{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings:\n",
    "In this notebook, we explore how to convert how preprocessed textual data into word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/spacy_preprocessed.pickle', 'rb') as file:\n",
    "\n",
    "    preprocessed_data= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972406"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> green and blue </s> <s> very thick and soft </s> <s> perfect for layer on cold day </s> <s> like new condition </s> <s> free shipping </s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try getting some vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree= nlp('tree')\n",
    "tree.has_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2919e-01, -1.2684e+00, -7.2209e+00, -1.6487e+00,  1.7131e+00,\n",
       "       -3.1834e+00,  5.6145e-01,  3.6603e+00, -9.6616e-02,  3.2192e+00,\n",
       "        2.4644e+00,  1.8412e+00, -5.7259e+00,  3.0492e+00, -8.4666e-01,\n",
       "       -1.7067e+00,  2.3521e+00, -3.3682e-01,  5.7802e+00, -3.7691e+00,\n",
       "        4.3651e+00,  7.4848e+00,  2.4060e-03,  2.9580e-01, -1.1311e+00,\n",
       "       -5.1604e+00, -5.8515e+00, -1.5563e+00,  7.7758e-01,  5.4768e+00,\n",
       "       -4.9496e+00, -1.8279e+00,  1.4919e+00, -5.6514e+00,  3.3848e+00,\n",
       "       -4.3007e+00, -2.1703e+00,  6.1267e+00,  2.7119e+00, -3.4822e-01,\n",
       "        2.8005e+00, -2.0020e+00,  3.3536e+00,  4.1416e+00,  3.0407e+00,\n",
       "        1.2859e+01,  3.3048e+00,  1.0003e-01,  4.2711e-01, -2.8046e+00,\n",
       "        3.8545e+00,  6.4176e+00, -9.4765e-02, -2.7159e+00, -2.4363e+00,\n",
       "        2.2896e+00, -1.4956e-01, -3.0021e+00,  5.0707e+00, -3.0722e+00,\n",
       "       -3.9061e-01, -3.0551e+00,  4.8163e+00,  2.6263e+00, -5.7205e+00,\n",
       "        3.2005e+00,  1.6785e+00, -1.9029e+00, -3.0395e+00,  1.6899e+00,\n",
       "       -2.0119e-01,  3.9220e+00,  6.3314e-02,  1.9296e+00,  2.9977e+00,\n",
       "        4.4304e-01, -2.2862e+00, -2.4761e+00,  1.9259e+00, -6.5126e-01,\n",
       "       -5.2685e+00, -3.0429e+00, -2.0036e+00, -5.5233e+00,  2.7961e+00,\n",
       "        6.8202e+00,  2.4157e+00,  1.6050e+01,  1.6609e+00,  3.0900e+00,\n",
       "        5.0486e+00, -3.6169e-01, -9.1365e-01, -1.4345e+00, -2.1376e+00,\n",
       "       -1.6020e+00,  1.1716e+01,  1.0534e+00,  1.4462e+00, -3.2564e+00,\n",
       "        4.7519e-01,  4.4497e+00,  7.6497e+00, -5.2617e+00, -3.5243e-01,\n",
       "        3.6918e+00, -4.5760e+00, -3.0525e+00, -2.7974e+00, -7.9331e-01,\n",
       "       -3.0214e+00,  5.6389e+00,  3.0073e+00,  7.2726e-01, -4.4103e+00,\n",
       "       -1.2224e+00,  4.2478e+00, -1.1228e+00,  6.7394e+00, -3.2783e+00,\n",
       "       -3.5062e-01, -3.3700e-01, -8.9118e+00,  4.0082e+00, -4.3905e+00,\n",
       "       -5.0397e+00,  4.0596e-01, -3.0445e+00,  5.4553e+00,  1.4952e+00,\n",
       "       -4.6685e+00,  4.0085e+00,  5.0192e+00, -8.2998e+00, -1.4068e+00,\n",
       "       -2.9764e+00, -4.4475e+00, -2.6943e+00, -9.6302e-01,  2.6309e+00,\n",
       "        1.4036e+00, -4.3574e+00,  2.6515e+00,  1.0734e+00,  2.6189e+00,\n",
       "       -4.6012e+00, -5.5647e+00,  1.4179e+00, -1.4583e+00,  3.0088e+00,\n",
       "       -7.8383e-01,  5.6317e+00,  9.9084e-01, -3.9565e+00,  1.4734e+00,\n",
       "        1.8239e+00, -9.3923e-01,  1.9408e+00,  5.4176e+00,  1.8521e+00,\n",
       "       -1.0312e+00, -3.6823e+00,  3.6943e+00,  2.5598e+00,  4.2870e+00,\n",
       "        4.7133e-01,  9.1680e-01,  5.7133e+00, -3.5855e+00,  3.8804e-01,\n",
       "       -3.1012e-01,  1.3574e+00,  4.9657e+00, -1.4754e+00,  5.1654e+00,\n",
       "       -3.4816e+00,  8.3221e-01,  1.8099e+00, -1.4290e+00, -1.5476e+00,\n",
       "        9.5085e-01,  2.7000e+00, -3.4331e+00,  2.7430e+00,  4.4859e-02,\n",
       "       -4.7033e+00,  5.0950e+00, -1.5251e+00,  5.4769e-01,  3.0059e-01,\n",
       "        4.8762e+00, -1.5896e+00, -5.6330e-01, -2.4314e+00, -3.8647e+00,\n",
       "       -4.4057e-02, -4.3887e+00,  5.4727e+00,  7.7038e-01, -2.3986e+00,\n",
       "        2.8133e+00, -6.7961e-01,  8.6864e-01,  2.0083e+00,  5.1675e+00,\n",
       "        5.8739e-01, -4.0884e+00, -6.1986e+00, -1.3156e+00, -4.0256e+00,\n",
       "       -2.3573e+00,  2.8748e+00, -9.1156e-01,  4.9855e+00,  7.6368e+00,\n",
       "        1.1159e+00,  6.3634e+00, -8.9455e-01,  9.0846e-01, -1.4423e+00,\n",
       "        8.8400e+00,  1.9202e-02, -1.6006e+00,  1.7805e+00,  9.6860e-02,\n",
       "        1.5209e+00,  3.0251e+00,  8.2442e+00,  6.4958e+00,  8.1782e+00,\n",
       "       -4.8614e+00, -5.3879e+00, -2.5568e+00,  3.0938e+00, -5.7449e+00,\n",
       "        1.0798e+00, -4.5318e+00,  6.4501e+00,  3.0887e+00, -3.8681e+00,\n",
       "        1.2506e+00,  2.0110e+00,  1.0260e+00,  8.2178e-02, -7.0821e-02,\n",
       "        3.1531e+00, -2.7286e+00,  1.2445e+00, -2.3140e-01, -2.3008e+00,\n",
       "       -2.3071e+00, -1.0476e+00, -3.1893e+00, -2.1946e-01, -1.9619e+00,\n",
       "       -2.3727e+00, -1.8968e+00,  5.9052e+00, -2.9860e+00,  1.5816e+00,\n",
       "       -1.5530e+00,  3.5088e+00, -1.8268e+00, -2.5549e+00, -4.8354e+00,\n",
       "       -2.3125e+00, -2.0711e+00,  5.7550e+00, -6.7864e+00,  6.3100e+00,\n",
       "        2.1556e+00, -3.4237e+00,  2.8218e+00, -5.1361e+00,  1.4398e+00,\n",
       "       -5.4292e+00, -2.3893e+00,  3.8220e-01, -5.2008e+00,  5.2818e+00,\n",
       "       -2.0524e+00,  3.9047e+00,  1.4363e-01,  3.5567e+00,  6.8210e+00,\n",
       "       -4.0768e+00, -3.8059e+00,  3.8801e+00, -2.6956e+00,  7.9555e-01,\n",
       "        1.5464e+00,  1.6893e+00,  5.2295e+00,  2.3481e+00, -1.1409e+00,\n",
       "       -5.0243e+00,  3.6412e+00, -1.8847e+00, -4.8449e-01, -3.9549e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a vector for a complete sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= nlp(preprocessed_data[0]).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = sent.reshape(1,-1)\n",
    "sent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert preprocessed text into feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(doc):\n",
    "\n",
    "    return nlp(doc).vector.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vectors for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of vectors created.\n"
     ]
    }
   ],
   "source": [
    "vector_list=[]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "\n",
    "    futures=[ executor.submit(get_vector,doc) for doc in preprocessed_data[:100] ] #assigning tasks to threads\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        vector_list.append(future.result()) # adding samples to list \n",
    " \n",
    "print(\"List of vectors created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.concatenate(vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential code\n",
    "# data_list = [nlp(doc).vector.reshape(1,-1) for doc in tqdm(preprocessed_data)]\n",
    "# data = np.concatenate(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorTransformer(TransformerMixin,BaseEstimator):\n",
    "    def __init__(self, model=\"en_core_web_lg\"):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        nlp = spacy.load(self.model)\n",
    "        return np.concatenate([nlp(doc).vector.reshape(1,-1) for doc in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = WordVectorTransformer()\n",
    "vs=transformer.fit_transform(preprocessed_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03646948cc1eca1aab1a6d754d674736dcb5138b87307cb760ddb3b102c41299"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
